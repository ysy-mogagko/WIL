# 딥러닝 역전파(Backpropagation)와 미분 이해하기

## 1. 역전파의 핵심 아이디어: 관찰 vs 계산

역전파의 목표는 **"가중치를 살짝 바꿨을 때 결과(오차)가 얼마나 변하는지"**를 알아내는 것입니다. 이 목표를 달성하는 방법에는 두 가지 접근이 있습니다.

-   **관찰법**: 가중치를 아주 미세하게 직접 바꿔보고, 그로 인해 오차 값이 실제로 얼마나 변하는지 관찰하는 방식입니다.
-   **계산법 (역전파의 방식)**: **미분(연쇄 법칙)**이라는 수학 공식을 이용해, '가중치를 바꿨을 때 오차가 얼마나 변할지'를 미리 계산해내는 방식입니다.

수백만 개의 가중치를 일일이 바꿔보며 관찰하는 것은 불가능하므로, 신경망은 모든 가중치의 영향력을 한 번의 계산으로 빠르고 정확하게 알아내는 '계산법'을 사용합니다.

---

## 2. 출력층 가중치의 기울기 계산: 연쇄 법칙 (Chain Rule)

출력층 가중치($W^{(2)}$)가 최종 오차($L$)에 미치는 영향력(기울기, $\frac{\partial L}{\partial W^{(2)}}$)을 구하는 과정입니다.

$W^{(2)}$는 오차 $L$에 직접 영향을 주지 않고, 여러 단계를 거칩니다.

$W^{(2)} \rightarrow z^{(2)} \rightarrow a^{(2)} \rightarrow L$

> **용어 정리**
> - **가중합 ($z^{(2)}$)**: 이전 층의 출력값($a^{(1)}$)과 가중치($W^{(2)}$)를 곱하고 편향($b^{(2)}$)을 더한 값. 뉴런에 들어온 신호를 종합한 중간 계산 결과입니다. ($z^{(2)} = W^{(2)}a^{(1)} + b^{(2)}$)
> - **예측값 ($a^{(2)}$)**: 가중합($z^{(2)}$)을 활성화 함수($\sigma$)에 통과시켜 최종적으로 얻은 신경망의 예측 결과입니다. ($a^{(2)} = \sigma(z^{(2)})$)

역전파는 이 계산 흐름을 거꾸로 따라가며 각 단계의 영향력(미분값)을 곱합니다.

$\frac{\partial L}{\partial W^{(2)}} = \frac{\partial L}{\partial a^{(2)}} \cdot \frac{\partial a^{(2)}}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial W^{(2)}}$

각 조각의 의미는 다음과 같습니다.

1.  **$\frac{\partial L}{\partial a^{(2)}}$ : 최종 예측값의 오차**
    -   **의미**: 예측값($a^{(2)}$)이 변할 때 최종 오차($L$)는 얼마나 변하는가?
    -   **계산**: 손실 함수 $L = \frac{1}{2}(y-a^{(2)})^2$ 를 $a^{(2)}$에 대해 미분하면 $(a^{(2)} - y)$가 됩니다.

2.  **$\frac{\partial a^{(2)}}{\partial z^{(2)}}$ : 활성화 함수의 민감도**
    -   **의미**: 활성화 함수에 들어가는 값($z^{(2)}$)이 변할 때 예측값($a^{(2)}$)은 얼마나 변하는가?
    -   **계산**: 활성화 함수($\sigma$)의 미분값, 즉 $\sigma'(z^{(2)})$입니다.

3.  **$\frac{\partial z^{(2)}}{\partial W^{(2)}}$ : 가중치의 직접적인 영향력**
    -   **의미**: 우리가 바꿀 수 있는 파라미터인 가중치($W^{(2)}$)를 바꿀 때 가중합($z^{(2)}$)은 얼마나 변하는가?
    -   **계산**: $z^{(2)} = W^{(2)}a^{(1)} + b^{(2)}$ 를 $W^{(2)}$에 대해 미분하면, 곱해져 있던 $a^{(1)}$만 남습니다.

이 세 조각을 모두 곱하면 우리가 원하던 기울기 공식이 완성됩니다.

$\frac{\partial L}{\partial W^{(2)}} = (a^{(2)} - y) \cdot \sigma'(z^{(2)}) \cdot a^{(1)}$

---

## 3. 간단한 미분 복습

-   **기본 규칙 ($x^n$ 미분)**: 지수(n)가 앞으로 내려와 곱해지고, 원래 지수에서 1을 뺀다.
    -   $x^2 \rightarrow 2x$
    -   $x \rightarrow 1$
    -   상수 $\rightarrow 0$

-   **미분의 정식 정의 (h 활용)**: 미분의 근본 원리는 두 점 사이의 평균 기울기에서, 두 점의 간격(h)을 0에 가깝게 보내 순간 기울기를 구하는 것입니다.
    $f'(x) = \lim_{h\to0} \frac{f(x+h) - f(x)}{h}$

---

## 4. 연습 문제

**문제**: 다음 신경망에서 $\frac{\partial L}{\partial W^{(2)}}$ 와 $\frac{\partial L}{\partial W^{(1)}}$ 를 계산해 보세요.

-   **구조**: 입력(1) -> 은닉(1) -> 출력(1)
-   **파라미터**: $x=2$, $W^{(1)}=3$, $W^{(2)}=4$, 실제 정답 $y=30$
-   **규칙**: 활성화 함수는 항등 함수($\sigma(x)=x$, 미분하면 1), 손실 함수는 $L = \frac{1}{2}(y-a^{(2)})^2$

**순전파 결과**:
-   $a^{(1)} = 6$
-   $a^{(2)} = 24$
-   $L = 18$

이제 역전파 공식을 이용해 기울기를 계산해볼 차례입니다.

---

**코멘트**: Chain rule의 당위성과 그것의 엄밀성에 대해서 생각해봐야하고, 만약 chain rule을 받아들인다면 그냥 계산 하면된다.
